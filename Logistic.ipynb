{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmqE2MxFI7DCk37R03qSQT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JitenVagadia/Sentiment-Prediction-Using-Logistic-Regression/blob/main/Logistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oXGzkdbMIe8",
        "outputId": "0052f3df-e984-41a0-9d40-0cf28a51661c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from google.colab import  drive\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df_train_pos = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/acllmdb/df_train_data_pos.csv')\n",
        "df_train_neg = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/acllmdb/df_train_data_neg.csv')\n",
        "df_test_pos = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/acllmdb/df_test_data_pos.csv')\n",
        "df_test_neg = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/acllmdb/df_test_data_neg.csv')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj6WtIfEOpln"
      },
      "source": [
        "df_train_pos['Sentiment'] = 1\n",
        "df_train_neg['Sentiment'] = 0\n",
        "df_test_pos['Sentiment'] = 1\n",
        "df_test_neg['Sentiment'] = 0\n",
        "df_train = pd.concat([df_train_pos,df_train_neg])\n",
        "df_test = pd.concat([df_test_pos,df_test_neg])\n",
        "df_train = df_train.sample(frac = 1) \n",
        "df_test = df_test.sample(frac = 1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgwT12yglzWG"
      },
      "source": [
        "def remove_punct(text):\n",
        "  text = \"\".join([char for char in text if char!=\"'\" ])\n",
        "  clean = re.sub(r\"\"\"[,.;@#?<>()\"\"-/!&$]+\\ *\"\"\",\" \",text, flags=re.VERBOSE)\n",
        "  return clean\n",
        "def remove_stop(text):\n",
        "  sw = stopwords.words(\"english\")\n",
        "  sw.append('br')\n",
        "  clean = \" \".join([x for x in text.split() if x not in sw])\n",
        "  return clean\n",
        "df_train['Tweets'] = df_train['Tweets'].apply(lambda tweet : remove_punct(tweet))\n",
        "df_test['Tweets'] = df_test['Tweets'].apply(lambda tweet : remove_punct(tweet))\n",
        "df_train['Tweets'] = df_train['Tweets'].apply(lambda tweet : remove_stop(tweet))\n",
        "df_test['Tweets'] = df_test['Tweets'].apply(lambda tweet : remove_stop(tweet))\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiMKk9d1RCQT",
        "outputId": "a56e69a4-ae21-4fa2-cad3-10b7762193ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vectorizer = CountVectorizer(min_df = 10)\n",
        "X = vectorizer.fit_transform(df_train['Tweets'])\n",
        "X = X.toarray()\n",
        "X = [x/sum(x) for x in X]\n",
        "y = [x for x in df_train['Sentiment']]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ruzedf-LGO6"
      },
      "source": [
        "test_X = vectorizer.transform(df_test['Tweets'])\n",
        "test_X = test_X.toarray()\n",
        "test_y = [x for x in df_test['Sentiment']]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGF8i0xVqt5o",
        "outputId": "b552c5cc-5186-43f0-d33a-02ad37397eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(len(vectorizer.get_feature_names()))\n",
        "print(len(X))\n",
        "print(len(y))\n",
        "print(len(X[0]))\n",
        "print(len(X[1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18831\n",
            "25000\n",
            "25000\n",
            "18831\n",
            "18831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg2TkPkBQFkK"
      },
      "source": [
        "clf = LogisticRegression().fit(X, y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enlGlSXnJzUs",
        "outputId": "37dff85b-90ed-44ff-d7fa-984e2a49cf3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "clf.score(test_X,test_y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.79116"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6qmFXZiMzKC",
        "outputId": "290ef9c6-9da5-49d3-fb29-4d87976437aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "clf.predict(vectorizer.transform(['bad movie dont watch miserable not worth time']))[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chauNTesNQPs",
        "outputId": "4803fe72-07f8-4bb6-a426-c1753fb2b8c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "clf.predict(vectorizer.transform(['very happy i found this gratetul very good and satisfying']))[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}